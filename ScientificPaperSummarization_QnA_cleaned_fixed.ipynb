{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Aasthajoshay/ScientificPaperSummarization-QnA/blob/main/ScientificPaperSummarization_QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O48QOWrHsTF",
    "outputId": "2f32215f-1957-4f7d-85b1-fab37a7cbda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/232.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "# prompt: install pypdf2, numpy,fitz,faiss\n",
    "\n",
    "!pip install pypdf2 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iom8YMEgIug3",
    "outputId": "418c43ec-d08c-4ce1-95b5-2e647db63a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvFF1wqGJlO8",
    "outputId": "66ba2cde-e7bc-4b72-dc20-08bab5c8c1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pymupdf\n",
    "\n",
    "# Now, try importing fitz again\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mBf3hRzcoRlV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from openai import OpenAI  # Groq uses OpenAI-compatible API\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Re-importing PDFCorpusManager from the previous cell to ensure it's available\n",
    "# when ScientificQASystem is defined in this new combined cell.\n",
    "# Alternatively, you could combine all classes into a single cell.\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "\n",
    "\n",
    "class PDFCorpusManager:\n",
    "    def __init__(self, corpus_directory=\"./pdf_corpus\", embeddings_path=\"./embeddings\"):\n",
    "        self.corpus_directory = Path(corpus_directory)\n",
    "        self.embeddings_path = Path(embeddings_path)\n",
    "        self.corpus_directory.mkdir(exist_ok=True)\n",
    "        self.embeddings_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Metadata storage\n",
    "        self.metadata = {}\n",
    "        self.chunks = []\n",
    "        self.index = None\n",
    "\n",
    "    def upload_pdf(self, pdf_file_path: str, metadata: Dict = None):\n",
    "        \"\"\"Upload and process a single PDF file\"\"\"\n",
    "        pdf_path = Path(pdf_file_path)\n",
    "\n",
    "        if not pdf_path.exists():\n",
    "            potential_path = Path(\"/content\") / pdf_path.name\n",
    "            if potential_path.exists():\n",
    "                 pdf_path = potential_path\n",
    "                 print(f\"File not found at {pdf_file_path}, found at {potential_path}\")\n",
    "            else:\n",
    "                 raise FileNotFoundError(f\"PDF file not found at {pdf_file_path} or {potential_path}\")\n",
    "\n",
    "\n",
    "        # Extract text from PDF\n",
    "        text_content = self._extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        # Generate unique document ID\n",
    "        doc_id = f\"doc_{len(self.metadata)}_{pdf_path.stem}\"\n",
    "\n",
    "        # Store metadata\n",
    "        self.metadata[doc_id] = {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"upload_date\": datetime.now().isoformat(),\n",
    "            \"file_path\": str(pdf_path),\n",
    "            \"text_length\": len(text_content),\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "\n",
    "        # Chunk the document\n",
    "        doc_chunks = self._chunk_document(text_content, doc_id)\n",
    "        self.chunks.extend(doc_chunks)\n",
    "\n",
    "        print(f\"✅ Uploaded and processed: {pdf_path.name}\")\n",
    "        print(f\"   - Generated {len(doc_chunks)} chunks\")\n",
    "        print(f\"   - Total corpus size: {len(self.chunks)} chunks\")\n",
    "\n",
    "        return doc_id\n",
    "\n",
    "    def _extract_text_from_pdf(self, pdf_path: Path) -> str:\n",
    "        \"\"\"Extracts text from a PDF file using PyMuPDF (fitz).\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            print(f\"Successfully extracted text from {pdf_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path.name}: {e}\")\n",
    "            text = \"\"\n",
    "        return text\n",
    "\n",
    "    def _chunk_document(self, text: str, doc_id: str, chunk_size: int = 512, overlap: int = 64) -> List[Dict]:\n",
    "        \"\"\"Split document into chunks with metadata\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "\n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk_text = \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "            chunk_data = {\n",
    "                \"text\": chunk_text,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_index\": len(chunks),\n",
    "                \"start_word\": i,\n",
    "                \"end_word\": min(i + chunk_size, len(words))\n",
    "            }\n",
    "            chunks.append(chunk_data)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def build_corpus_index(self):\n",
    "        \"\"\"Build FAISS index for the entire corpus\"\"\"\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No documents in corpus. Upload PDFs first.\")\n",
    "\n",
    "        print(\"🔄 Building corpus embeddings...\")\n",
    "\n",
    "        chunk_texts = [chunk[\"text\"] for chunk in self.chunks]\n",
    "\n",
    "        embeddings = self.embedding_model.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "        print(f\"✅ Built corpus index with {len(chunk_texts)} chunks\")\n",
    "\n",
    "        self._save_corpus()\n",
    "\n",
    "    def _save_corpus(self):\n",
    "        \"\"\"Save corpus index and metadata to disk\"\"\"\n",
    "        faiss.write_index(self.index, str(self.embeddings_path / \"corpus.index\"))\n",
    "\n",
    "        corpus_data = {\n",
    "            \"metadata\": self.metadata,\n",
    "            \"chunks\": self.chunks,\n",
    "            \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "            \"created_date\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(self.embeddings_path / \"corpus_data.json\", \"w\") as f:\n",
    "            json.dump(corpus_data, f, indent=2)\n",
    "\n",
    "        print(f\"💾 Saved corpus to {self.embeddings_path}\")\n",
    "\n",
    "    def load_corpus(self):\n",
    "        \"\"\"Load existing corpus from disk\"\"\"\n",
    "        index_path = self.embeddings_path / \"corpus.index\"\n",
    "        data_path = self.embeddings_path / \"corpus_data.json\"\n",
    "\n",
    "        if not (index_path.exists() and data_path.exists()):\n",
    "            print(\"⚠️  No existing corpus found. Upload PDFs and build index first.\")\n",
    "            return False\n",
    "\n",
    "        self.index = faiss.read_index(str(index_path))\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            corpus_data = json.load(f)\n",
    "\n",
    "        self.metadata = corpus_data[\"metadata\"]\n",
    "        self.chunks = corpus_data[\"chunks\"]\n",
    "\n",
    "        print(f\"✅ Loaded corpus with {len(self.chunks)} chunks from {len(self.metadata)} documents\")\n",
    "        return True\n",
    "\n",
    "    def search_corpus(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search corpus for relevant chunks\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Corpus index not built. Call build_corpus_index() first.\")\n",
    "\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            chunk[\"similarity_score\"] = float(distances[0][i])\n",
    "            chunk[\"document_info\"] = self.metadata[chunk[\"doc_id\"]]\n",
    "            results.append(chunk)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_corpus_stats(self):\n",
    "        \"\"\"Get statistics about the corpus\"\"\"\n",
    "        return {\n",
    "            \"total_documents\": len(self.metadata),\n",
    "            \"total_chunks\": len(self.chunks),\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"filename\": info[\"filename\"],\n",
    "                    \"upload_date\": info[\"upload_date\"],\n",
    "                    \"text_length\": info[\"text_length\"]\n",
    "                }\n",
    "                for doc_id, info in self.metadata.items()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class ScientificQASystem:\n",
    "    def __init__(self, corpus_manager: PDFCorpusManager):\n",
    "        self.corpus_manager = corpus_manager\n",
    "\n",
    "        # Get Groq API key from Colab secrets\n",
    "        try:\n",
    "            groq_api_key = userdata.get('GROQ_API_KEY')\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in Colab secrets. Please add it in the secrets panel.\")\n",
    "\n",
    "        # Initialize Groq client (uses OpenAI-compatible interface)\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=groq_api_key\n",
    "        )\n",
    "\n",
    "        print(\"✅ Groq API client initialized successfully\")\n",
    "\n",
    "    def build_few_shot_prompt(self, user_question: str, retrieved_chunks: List[Dict]) -> str:\n",
    "        \"\"\"Build few-shot prompt with retrieved context from corpus\"\"\"\n",
    "\n",
    "        context_parts = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            doc_info = chunk[\"document_info\"]\n",
    "            context_parts.append(f\"[From {doc_info['filename']}]: {chunk['text']}\")\n",
    "\n",
    "        combined_context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "        prompt = f\"\"\"You are a research assistant analyzing a corpus of scientific journal articles. Given a question and relevant context from multiple papers, provide a concise, accurate answer based only on the provided context.\n",
    "\n",
    "Example 1:\n",
    "Question: What methodology was used to collect data about AI tool usage in the study?\n",
    "Context: [From paper1.pdf]: Data for the study were collected using a structured questionnaire distributed during the final team presentation of the programming project. The questionnaire was structured to comprehensively evaluate the adoption and application of AI tools during the programming project, following the Technology Acceptance Model.\n",
    "Answer: The researchers used a structured questionnaire distributed during final team presentations, following the Technology Acceptance Model framework to evaluate AI tool adoption and application.\n",
    "\n",
    "Example 2:\n",
    "Question: What was the adoption rate of AI tools among students in the programming project?\n",
    "Context: [From paper1.pdf]: Of the 38 participants in this study, 34 used AI tools during the programming project, resulting in an adoption rate of 89.4%. The remaining four participants all reported that they did not use AI tools, citing either a lack of need or personal reservations.\n",
    "Answer: The AI tool adoption rate was 89.4%, with 34 out of 38 participants using AI tools during the programming project.\n",
    "Example 3:\n",
    "Question: Which development phase had the highest AI tool usage?\",\n",
    "Context: [From paper1.pdf]: The Coding & Testing phase had the highest adoption rate, with 33 participants using AI tools and only 5 opting out. CHATGPT was used by all students who used AI tools, regardless of the phase.\",\n",
    "Answer: The Coding & Testing phase had the highest AI tool usage with 33 out of 38 participants using AI tools.\n",
    "\n",
    "Now answer the following:\n",
    "\n",
    "Question: {user_question}\n",
    "Context: {combined_context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def answer_question(self, question: str, num_chunks: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Answer a question using the corpus with Groq API\"\"\"\n",
    "\n",
    "        # Step 1: Retrieve relevant chunks from corpus\n",
    "        print(f\"🔍 Searching corpus for: {question}\")\n",
    "        retrieved_chunks = self.corpus_manager.search_corpus(question, k=num_chunks)\n",
    "\n",
    "        # Step 2: Build few-shot prompt\n",
    "        prompt = self.build_few_shot_prompt(question, retrieved_chunks)\n",
    "\n",
    "        # Step 3: Generate answer using Groq API\n",
    "        print(\"🤖 Generating answer with Groq...\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",  # Fast Groq model\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200,\n",
    "                temperature=0.1,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with Groq API: {e}\")\n",
    "            answer = \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "        # Return comprehensive result\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"document\": chunk[\"document_info\"][\"filename\"],\n",
    "                    \"similarity_score\": chunk[\"similarity_score\"],\n",
    "                    \"text_snippet\": chunk[\"text\"][:200] + \"...\"\n",
    "                }\n",
    "                for chunk in retrieved_chunks\n",
    "            ],\n",
    "            \"num_sources\": len(retrieved_chunks),\n",
    "            \"model_used\": \"llama3-8b-8192 (Groq)\"\n",
    "        }\n",
    "\n",
    "    def batch_answer_questions(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Answer multiple questions in batch\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\n📝 Processing question {i}/{len(questions)}\")\n",
    "            result = self.answer_question(question)\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class QAMetrics:\n",
    "    def __init__(self):\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def exact_match(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Exact Match Score - strict character-level matching\"\"\"\n",
    "        predicted = predicted.strip().lower()\n",
    "        ground_truth = ground_truth.strip().lower()\n",
    "        return 1.0 if predicted == ground_truth else 0.0\n",
    "\n",
    "    def f1_score_qa(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Token-level F1 Score for QA\"\"\"\n",
    "        def normalize_text(text):\n",
    "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "            return set(text.split())\n",
    "\n",
    "        pred_tokens = normalize_text(predicted)\n",
    "        truth_tokens = normalize_text(ground_truth)\n",
    "\n",
    "        if not truth_tokens:\n",
    "            return 1.0 if not pred_tokens else 0.0\n",
    "\n",
    "        common_tokens = pred_tokens.intersection(truth_tokens)\n",
    "\n",
    "        if not common_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    def semantic_similarity(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Semantic similarity using sentence embeddings\"\"\"\n",
    "        pred_embedding = self.sentence_model.encode([predicted])\n",
    "        truth_embedding = self.sentence_model.encode([ground_truth])\n",
    "\n",
    "        similarity = cosine_similarity(pred_embedding, truth_embedding)[0][0]\n",
    "        return max(0.0, similarity)\n",
    "\n",
    "    def answer_relevancy(self, question: str, answer: str) -> float:\n",
    "        \"\"\"Measure how relevant the answer is to the question\"\"\"\n",
    "        question_embedding = self.sentence_model.encode([question])\n",
    "        answer_embedding = self.sentence_model.encode([answer])\n",
    "\n",
    "        relevancy = cosine_similarity(question_embedding, answer_embedding)[0][0]\n",
    "        return max(0.0, relevancy)\n",
    "\n",
    "\n",
    "class QAEvaluator:\n",
    "    def __init__(self, qa_system: ScientificQASystem):\n",
    "        self.qa_system = qa_system\n",
    "        self.evaluation_data = []\n",
    "\n",
    "    def create_evaluation_dataset(self):\n",
    "        \"\"\"Create ground truth Q&A pairs for evaluation\"\"\"\n",
    "        evaluation_questions = [\n",
    "            {\n",
    "                \"question\": \"What methodology was used to collect data in the AI tools study?\",\n",
    "                \"ground_truth\": \"A structured questionnaire was distributed during final team presentations, following the Technology Acceptance Model framework.\",\n",
    "                \"category\": \"methodology\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What was the adoption rate of AI tools among students?\",\n",
    "                \"ground_truth\": \"89.4% adoption rate, with 34 out of 38 participants using AI tools.\",\n",
    "                \"category\": \"statistics\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Which development phase had the highest AI tool usage?\",\n",
    "                \"ground_truth\": \"The Coding & Testing phase had the highest usage with 33 out of 38 participants.\",\n",
    "                \"category\": \"findings\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.evaluation_data = evaluation_questions\n",
    "        return evaluation_questions\n",
    "\n",
    "    def evaluate_retrieval_quality(self, question: str, ground_truth_docs: List[str], k: int = 5) -> Dict:\n",
    "        \"\"\"Evaluate retrieval component performance\"\"\"\n",
    "        retrieved_chunks = self.qa_system.corpus_manager.search_corpus(question, k=k)\n",
    "        retrieved_doc_names = [chunk[\"document_info\"][\"filename\"] for chunk in retrieved_chunks]\n",
    "\n",
    "        # Calculate Recall@K\n",
    "        relevant_retrieved = sum(1 for doc in ground_truth_docs if doc in retrieved_doc_names)\n",
    "        recall_at_k = relevant_retrieved / len(ground_truth_docs) if ground_truth_docs else 0.0\n",
    "\n",
    "        # Calculate Mean Reciprocal Rank (MRR)\n",
    "        mrr = 0.0\n",
    "        for i, doc_name in enumerate(retrieved_doc_names):\n",
    "            if doc_name in ground_truth_docs:\n",
    "                mrr = 1.0 / (i + 1)\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"recall_at_k\": recall_at_k,\n",
    "            \"mrr\": mrr,\n",
    "            \"retrieved_docs\": retrieved_doc_names,\n",
    "            \"num_relevant_retrieved\": relevant_retrieved\n",
    "        }\n",
    "\n",
    "    def run_comprehensive_evaluation(self):\n",
    "        \"\"\"Run complete evaluation of the QA system\"\"\"\n",
    "\n",
    "        print(\"🔍 Starting Comprehensive Evaluation...\")\n",
    "\n",
    "        metrics = QAMetrics()\n",
    "        results = {\n",
    "            \"exact_match_scores\": [],\n",
    "            \"f1_scores\": [],\n",
    "            \"semantic_similarity_scores\": [],\n",
    "            \"answer_relevancy_scores\": [],\n",
    "            \"retrieval_metrics\": [],\n",
    "            \"detailed_results\": []\n",
    "        }\n",
    "\n",
    "        evaluation_data = self.create_evaluation_dataset()\n",
    "\n",
    "        for i, eval_item in enumerate(evaluation_data):\n",
    "            print(f\"\\n📝 Evaluating question {i+1}/{len(evaluation_data)}\")\n",
    "\n",
    "            question = eval_item[\"question\"]\n",
    "            ground_truth = eval_item[\"ground_truth\"]\n",
    "\n",
    "            # Generate answer\n",
    "            qa_result = self.qa_system.answer_question(question)\n",
    "            predicted_answer = qa_result[\"answer\"]\n",
    "\n",
    "            # Calculate metrics\n",
    "            em_score = metrics.exact_match(predicted_answer, ground_truth)\n",
    "            f1_score = metrics.f1_score_qa(predicted_answer, ground_truth)\n",
    "            semantic_score = metrics.semantic_similarity(predicted_answer, ground_truth)\n",
    "            relevancy_score = metrics.answer_relevancy(question, predicted_answer)\n",
    "\n",
    "            # Store results\n",
    "            results[\"exact_match_scores\"].append(em_score)\n",
    "            results[\"f1_scores\"].append(f1_score)\n",
    "            results[\"semantic_similarity_scores\"].append(semantic_score)\n",
    "            results[\"answer_relevancy_scores\"].append(relevancy_score)\n",
    "\n",
    "            detailed_result = {\n",
    "                \"question\": question,\n",
    "                \"predicted_answer\": predicted_answer,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"exact_match\": em_score,\n",
    "                \"f1_score\": f1_score,\n",
    "                \"semantic_similarity\": semantic_score,\n",
    "                \"answer_relevancy\": relevancy_score,\n",
    "                \"category\": eval_item[\"category\"]\n",
    "            }\n",
    "            results[\"detailed_results\"].append(detailed_result)\n",
    "\n",
    "            print(f\"   EM: {em_score:.3f} | F1: {f1_score:.3f} | Semantic: {semantic_score:.3f}\")\n",
    "\n",
    "        # Calculate averages\n",
    "        avg_results = {\n",
    "            \"avg_exact_match\": np.mean(results[\"exact_match_scores\"]),\n",
    "            \"avg_f1_score\": np.mean(results[\"f1_scores\"]),\n",
    "            \"avg_semantic_similarity\": np.mean(results[\"semantic_similarity_scores\"]),\n",
    "            \"avg_answer_relevancy\": np.mean(results[\"answer_relevancy_scores\"])\n",
    "        }\n",
    "\n",
    "        return results, avg_results\n",
    "\n",
    "    def generate_evaluation_report(self, results: Dict, avg_results: Dict):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 SCIENTIFIC QA SYSTEM EVALUATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\n🎯 Overall Performance Metrics:\")\n",
    "        print(f\"   • Average Exact Match:      {avg_results['avg_exact_match']:.3f}\")\n",
    "        print(f\"   • Average F1 Score:         {avg_results['avg_f1_score']:.3f}\")\n",
    "        print(f\"   • Average Semantic Sim:     {avg_results['avg_semantic_similarity']:.3f}\")\n",
    "        print(f\"   • Average Answer Relevancy: {avg_results['avg_answer_relevancy']:.3f}\")\n",
    "\n",
    "        # Performance by category\n",
    "        categories = {}\n",
    "        for result in results[\"detailed_results\"]:\n",
    "            cat = result[\"category\"]\n",
    "            if cat not in categories:\n",
    "                categories[cat] = {\"f1_scores\": [], \"semantic_scores\": []}\n",
    "            categories[cat][\"f1_scores\"].append(result[\"f1_score\"])\n",
    "            categories[cat][\"semantic_scores\"].append(result[\"semantic_similarity\"])\n",
    "\n",
    "        print(f\"\\n📈 Performance by Question Category:\")\n",
    "        for category, scores in categories.items():\n",
    "            avg_f1 = np.mean(scores[\"f1_scores\"])\n",
    "            avg_semantic = np.mean(scores[\"semantic_scores\"])\n",
    "            print(f\"   • {category.title()}: F1={avg_f1:.3f}, Semantic={avg_semantic:.3f}\")\n",
    "\n",
    "        # Best and worst performing questions\n",
    "        sorted_results = sorted(results[\"detailed_results\"],\n",
    "                              key=lambda x: x[\"f1_score\"], reverse=True)\n",
    "\n",
    "        print(f\"\\n✅ Best Performing Question:\")\n",
    "        best = sorted_results[0]\n",
    "        print(f\"   Q: {best['question'][:80]}...\")\n",
    "        print(f\"   F1: {best['f1_score']:.3f}\")\n",
    "\n",
    "        print(f\"\\n❌ Worst Performing Question:\")\n",
    "        worst = sorted_results[-1]\n",
    "        print(f\"   Q: {worst['question'][:80]}...\")\n",
    "        print(f\"   F1: {worst['f1_score']:.3f}\")\n",
    "\n",
    "        return {\n",
    "            \"overall_metrics\": avg_results,\n",
    "            \"category_performance\": categories,\n",
    "            \"best_question\": best,\n",
    "            \"worst_question\": worst\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oSRn5tZZoubP"
   },
   "outputs": [],
   "source": [
    "def answer_question(self, question: str, num_chunks: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Answer a question using the corpus with Groq API\"\"\"\n",
    "\n",
    "        # Step 1: Retrieve relevant chunks from corpus\n",
    "        print(f\"🔍 Searching corpus for: {question}\")\n",
    "        retrieved_chunks = self.corpus_manager.search_corpus(question, k=num_chunks)\n",
    "\n",
    "        # Step 2: Build few-shot prompt\n",
    "        prompt = self.build_few_shot_prompt(question, retrieved_chunks)\n",
    "\n",
    "        # Step 3: Generate answer using Groq API\n",
    "        print(\"🤖 Generating answer with Groq...\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",  # Fast Groq model\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=200,\n",
    "                temperature=0.1,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with Groq API: {e}\")\n",
    "            answer = \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "        # Return comprehensive result\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"document\": chunk[\"document_info\"][\"filename\"],\n",
    "                    \"similarity_score\": chunk[\"similarity_score\"],\n",
    "                    \"text_snippet\": chunk[\"text\"][:200] + \"...\"\n",
    "                }\n",
    "                for chunk in retrieved_chunks\n",
    "            ],\n",
    "            \"num_sources\": len(retrieved_chunks),\n",
    "            \"model_used\": \"llama3-8b-8192 (Groq)\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sK-LSwSBs-mO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Retrieve the Groq API Key securely from Colab's Userdata secrets\n",
    "try:\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "    # Set the environment variable for the Groq client to use\n",
    "    os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "except SecretNotFoundError:\n",
    "    print(\"Error: Secret 'GROQ_API_KEY' not found in Colab secrets.\")\n",
    "    print(\"Please add your Groq API Key to the Colab secrets manager\")\n",
    "    print(\"Click the key icon in the left sidebar.\")\n",
    "    GROQ_API_KEY = None # Ensure GROQ_API_KEY is None if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IWBz-6kuHxH",
    "outputId": "69ebffde-1271-46f2-f24d-0ddb092d4477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the API key is loaded\n",
    "if not GROQ_API_KEY:\n",
    "    print(\"Error: GROQ_API_KEY not found. Please set it in your environment or .env file.\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3eedc243d7bd4dca96fca15e8523c763",
      "6147b455d4004d59a7de97aec38e3f43",
      "0420a01c8e5c4d5daef42dece58f8d4b",
      "68beddf483104c8589ab70be2499c689",
      "965573e0b9e647069ca1ef33c1f49224",
      "60e24dccb1614634a18bfe20478debc8",
      "57b20e8b80514c6d9d5f417d5d227774",
      "f48c3bb1bf34472a922d288bc63f2301",
      "a87d4260148e4f2c808b86a29b7f88c8",
      "1becfe4ac4c145d78402cc412f3090c3",
      "554c4d1817484b3f9cbef4b67ddc5a24",
      "f4601e7bc1014fc8873c4edfefb0dc51",
      "af95ecb786324caa8f9107d3d27b772d",
      "8f7754952cd040749122f921276f050d",
      "52c052b120f74fe5b62240f4093fdff1",
      "0cb1509a6fe04edb8712c30bf445f9e7",
      "baf0a7a63d5c4d478bd8226d1d76d961",
      "45087823189b4e1190340efa12788d76",
      "671effa369064e2ea78fca1a804904af",
      "7bc1963a645f49929fc053c979d68306",
      "b15ebd0bc1ce46d69f9df67926c5061a",
      "35726ee9ef5049dea806be1528c4eb24",
      "2c4a3e869576449393540a44b8138358",
      "00d53f8fa465489886e60b5c901c8b56",
      "70cdeda55c1b4edeaf067cb1a085f810",
      "945e7854fb9044feadf5dbe0ff3ca02f",
      "85c398debee64e42b9022a45af5aa353",
      "2e1ae32e38d5421b87922a4615c68f14",
      "0b918b465fa0435a818f27c28300eab0",
      "8c3051e061884bb3baca2731aab1bbea",
      "ce88364ef4ab4bffaa2b298414144fb9",
      "7364cd4550734c25ad03a5b3ccbc8e13",
      "a27cd55844834eadaad1277245a3e8ba",
      "857adc0c1ab248c2a26c8447bd0569b5",
      "0b41b486d358443daba23ab27923f1d8",
      "099c605d4bc44198baac8c33631c1153",
      "aabd7ca31524485384fa09f4a438fd4a",
      "e7f9de1b6f5446d5b7e8985c1b85513b",
      "5add1855b98646148bf0bb6ba3b3a179",
      "6ddc07930a4f40799f4dc0cd90a540e1",
      "82d98b631c48400085c1c4623d35c685",
      "d1e2891583cb4810906af2288fa6cf63",
      "b3c008d15a754a6292260c24806cf8ef",
      "983f9cac3c8e4328a18254d02fd69398",
      "0e13b2bb62ba474ca07bf2d99ae1c9f0",
      "f4b122751dfb4dfdb09c3ba1b1864677",
      "6d92d118352d429bad5d11e4f716157d",
      "e3e8ed37a9c046f7ab3a2bace1cf8818",
      "73a20a46a5cd4942b97290c7d58e65a2",
      "08a49a59fb2044b8a0e97e606564b0af",
      "f804ce695b584f53bc5fbed8ef22c7ef",
      "86228126f9674a73913f396f8ddaa2c9",
      "70b3253c17cb43e2bbba24ac70244d5a",
      "03766dc683a44dd19145d6adf83f3b1a",
      "42c89ccc4be94b1d89e3af30cc8a478e",
      "a513e0cdb0b64229be8feda926a3a68c",
      "9c5accbd8ca540bf9d9c548fbffd402e",
      "b3c7395c28b34c9ca198400b75461ad8",
      "f15a030b2a6c4294b4b082b643759f00",
      "c80259f0bfbb43719515e61f69ca5690",
      "f54553d0e7134bfb9b96e398d0a87dba",
      "3c58c5b609c643fdb59b79fe2621b27f",
      "c0c0863c43ea433aa6eec8018bff3beb",
      "177963701c4e4dde81e8cc658118f3c6",
      "b274519a0a534f3986b24cc7c14da741",
      "5ef4bfc6b1d146829561f4a62aa1d973",
      "7458e2eb13ff4ffbb66d3411e4f273df",
      "c88e0ed99faa402a8d180a1dab82f9c4",
      "dd0c3d996b2f4abf90ea082218b1939f",
      "4897e82155ec4bdd986d6cf54829fbcb",
      "3c83b202034d41b3832a971c7a0a0fec",
      "a2c470f2f17c429ba50352dc18af7630",
      "7e66d861df2e492f9129bfb7a4be1e46",
      "6418bcfaba6947bab7d3dac9081f1116",
      "3b8935fd7b164ed6a9b84d9416d829c1",
      "2022a63ea8aa4babbd4539d8907cc1ce",
      "75843a53d5a24bc7993979f84057f033",
      "b05fd50dd8934100b87edb23e5bf31f4",
      "973cb20ea0994847bb7c7bd22dd0d169",
      "dca41eef3f914ef6a4d863edb33b4d7d",
      "2e06c602dfc24e8c844ab5ab4c24e881",
      "698e51de673e4b1c83be0ff2df0d47bc",
      "1da376f612544dec80e9006aff17be45",
      "49f606693b9a4f2896edc26392ade207",
      "715da11024a94b2a814c5d436cbef186",
      "38292a36f51149389414cf78557d22ec",
      "dd2c8b80d4d5495aa219705b5c076ffc",
      "543d2cee27f54618aca6e3d1723e9294",
      "0dbad4d9feee49458d8f2129328dfdaf",
      "634a8276cad54780826563cdca528bb2",
      "d23b86202c3741bd8aba55a208b1d7d9",
      "d6481c22bff94cbea89a32d3fca4b46b",
      "c0fd0d8b808f4983b6fa005d1d320654",
      "555082f63b6942edadc07c499567897e",
      "8b1b279265f94d9aa8923ef9fca80b26",
      "8b081b00d93041a1a3960f93c5a753c4",
      "dd55e72032b248aea11bed417c7ac9f8",
      "a919ed907d3941b19474d4532726001b",
      "1fd05b0ba20f4fc0a1f40eb656ada4bb",
      "02f92ab035bf47ab9d456664c9406db4",
      "a43548eba9f840fbbee1853c1098d8d8",
      "f1b0cbf725184ed98f3504a803b75726",
      "f4d8be21864b41bb88a7d521de07d31d",
      "5670888800bf4b9281fae1fa8fa54775",
      "5f097921c8974c6285a9e21148815406",
      "aacf61148f1d481f88affbf42a3d38c1",
      "2cdd1de26b6743aa9073dc18e78d7b07",
      "3ca2a93b00c84ddc84d943e958bbce42",
      "be545f7bba1f46e483320b0877a97fe3",
      "318e7360ccb344549726232db1b6da22",
      "684435ae74b0424ea8f1664c7da82708",
      "80da6af6886f4bd3ae5010f0b1fd079a",
      "6dbcd60dbf0b4bc3997d864b09d968d6",
      "574c4be129f24ad998334b41f288e530",
      "ba796ef47c7c4937bba7ad53b7a9d64b",
      "6181ceba3fa946e49891f22fff9b45ea",
      "8a676efa28ff438c8e703d523fae43e5",
      "3fe921fd544b450b829896e44b1748c2",
      "d61558e0de344547bba4fb7934c3f714",
      "10d7a70e9a9a46b2956b911e5330c4e5",
      "af1f91fa2c2f45388991c884715b04e1",
      "76b17dc4203045828b0a814030ec0ef5",
      "18ebe66baa7e4cd19238042db22430be",
      "48ced3382bab4188845c14a4d9206d6b",
      "96b0fb8fe6c24cd5b179e22c320f27b9",
      "2019e2f49d624f68afa45077335cefc3",
      "730af467c3bf4b9c81237bf90c8e9d5d",
      "6fe38d5aa3c54fd9ab8d99b8e58e5f3c",
      "74eca232be774391bcb3627a84e6661a",
      "a616737e8f5b426f8460a6139afc3ff2",
      "8d142e9e81e14fbd81fc3876ba4e132c",
      "8329e29f697b40378cbeb16d2086627a"
     ]
    },
    "id": "s1M9i__8m1W5",
    "outputId": "5ecb42f0-620e-44c8-8cbc-42b63d1cc92d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eedc243d7bd4dca96fca15e8523c763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4601e7bc1014fc8873c4edfefb0dc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4a3e869576449393540a44b8138358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857adc0c1ab248c2a26c8447bd0569b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e13b2bb62ba474ca07bf2d99ae1c9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a513e0cdb0b64229be8feda926a3a68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7458e2eb13ff4ffbb66d3411e4f273df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05fd50dd8934100b87edb23e5bf31f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbad4d9feee49458d8f2129328dfdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f92ab035bf47ab9d456664c9406db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684435ae74b0424ea8f1664c7da82708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq API client initialized successfully\n",
      "📚 Building Scientific Paper Corpus...\n",
      "Successfully extracted text from 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf\n",
      "✅ Uploaded and processed: 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf\n",
      "   - Generated 21 chunks\n",
      "   - Total corpus size: 21 chunks\n",
      "Successfully extracted text from 2406.00560v1.pdf\n",
      "✅ Uploaded and processed: 2406.00560v1.pdf\n",
      "   - Generated 16 chunks\n",
      "   - Total corpus size: 37 chunks\n",
      "Successfully extracted text from Elsevier_Article__elsarticle__Template1.pdf\n",
      "✅ Uploaded and processed: Elsevier_Article__elsarticle__Template1.pdf\n",
      "   - Generated 0 chunks\n",
      "   - Total corpus size: 37 chunks\n",
      "Successfully extracted text from GenAI___Productivity.pdf\n",
      "✅ Uploaded and processed: GenAI___Productivity.pdf\n",
      "   - Generated 16 chunks\n",
      "   - Total corpus size: 53 chunks\n",
      "Successfully extracted text from applsci-15-05785.pdf\n",
      "✅ Uploaded and processed: applsci-15-05785.pdf\n",
      "   - Generated 0 chunks\n",
      "   - Total corpus size: 53 chunks\n",
      "🔄 Building corpus embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b17dc4203045828b0a814030ec0ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Built corpus index with 53 chunks\n",
      "💾 Saved corpus to embeddings\n",
      "\n",
      "📊 Corpus Statistics:\n",
      "   - Total Documents: 5\n",
      "   - Total Chunks: 53\n",
      "\n",
      "🔬 Answering Scientific Questions...\n",
      "🔍 Searching corpus for: What challenges did students face when using AI tools in software development?\n",
      "🤖 Generating answer with Groq...\n",
      "\n",
      "❓ Question: What challenges did students face when using AI tools in software development?\n",
      "✅ Answer: What challenges did students face when using AI tools in software development?\n",
      "\n",
      "According to the context, students faced several challenges when using AI tools in software development, including:\n",
      "\n",
      "* Inaccurate results: Students reported encountering inaccurate results when using AI tools for tasks such as code generation, debugging, and comprehension.\n",
      "* Missing context: Students mentioned that AI tools often lacked context or failed to understand domain-specific frameworks, leading to incorrect outputs.\n",
      "* Limitations in handling complex code bases: Students noted that AI tools struggled with complex code bases and required significant corrections to AI-suggested code snippets.\n",
      "* Difficulty in framing complex code problems: Students reported difficulties in framing complex code problems or generating contextually relevant methods.\n",
      "* Incorrect output during debugging: Students reported incorrect output during debugging, including instances where the AI \"hallucinated\" by generating nonsensical code.\n",
      "* Poorly defined tasks: Students noted that poorly defined tasks for the AI resulted in outputs that did not meet the intended goals.\n",
      "* Nonsensical tests:\n",
      "📄 Sources: 5 documents\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 0.616)\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 0.729)\n",
      "🔍 Searching corpus for: How effective are AI tools in improving coding productivity?\n",
      "🤖 Generating answer with Groq...\n",
      "\n",
      "❓ Question: How effective are AI tools in improving coding productivity?\n",
      "✅ Answer: How effective are AI tools in improving coding productivity?\n",
      "\n",
      "According to the provided context, the studies suggest that AI tools can enhance productivity in software development primarily through automated code generation. The use of AI tools, such as GitHub Copilot, CodeWhisperer, and ChatGPT, can automate routine tasks, increase focus, and minimize disruptions, ultimately enhancing productivity. The studies also highlight the positive correlation between the use of AI tools and the productivity of developers, with a focus on the automation of repetitive tasks. Additionally, the studies suggest that AI tools can improve efficiency and flow, allowing developers to create relevant and insightful content, such as reports, code, or design models, with less effort.\n",
      "📄 Sources: 5 documents\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 0.705)\n",
      "   - 2406.00560v1.pdf (similarity: 0.743)\n",
      "🔍 Searching corpus for: What methodologies were used to evaluate student performance?\n",
      "🤖 Generating answer with Groq...\n",
      "\n",
      "❓ Question: What methodologies were used to evaluate student performance?\n",
      "✅ Answer: The methodologies used to evaluate student performance were not explicitly mentioned in the provided context. However, it can be inferred that the course did not provide a formal evaluation of individual performance, as stated in Section 3.3: \"Participants do not receive a grade at the end of the course because it is difficult to evaluate the individual performance of the participants.\" Instead, the course focused on successful participation, and the public presentation of one's own game was considered a highlight of the course.\n",
      "📄 Sources: 5 documents\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 1.221)\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 1.243)\n",
      "🔍 Searching corpus for: What are the main factors influencing AI tool adoption in education?\n",
      "🤖 Generating answer with Groq...\n",
      "\n",
      "❓ Question: What are the main factors influencing AI tool adoption in education?\n",
      "✅ Answer: The main factors influencing AI tool adoption in education, as identified in the study, are:\n",
      "\n",
      "1. Task type: AI tools were more likely to be used for tasks that involved text-heavy activities, such as requirements specification, documentation, and code understanding, rather than tasks that required diagram generation or GUI design.\n",
      "2. Phase of the project: AI tool adoption was highest in the Coding & Testing phase, where students reported using AI tools to generate code, improve code, and understand code.\n",
      "3. Challenges: Students reported encountering challenges when using AI tools, including inaccurate results, missing context, and difficulties in integrating AI-generated code with existing code bases.\n",
      "4. Availability and encouragement: The study suggests that the availability and encouragement of AI tools in the course may have driven their adoption, as students were more likely to use AI tools if they were readily available and encouraged by the course instructors.\n",
      "5. Task complexity: Students reported that AI tools were more effective for simpler tasks, such as code generation and documentation,\n",
      "📄 Sources: 5 documents\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 0.691)\n",
      "   - 2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf (similarity: 0.705)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "corpus_manager = PDFCorpusManager(corpus_directory=\"./scientific_papers\")\n",
    "qa_system = ScientificQASystem(corpus_manager)\n",
    "\n",
    "# Upload multiple PDFs to build corpus\n",
    "pdf_files = [\n",
    "    \"2025ECSEEGenerativeAIinStudentSoftwareDevelopmentProjects.pdf\",\n",
    "    \"2406.00560v1.pdf\",\n",
    "    \"Elsevier_Article__elsarticle__Template1.pdf\",\n",
    "    \"GenAI___Productivity.pdf\", # Added missing comma here\n",
    "    \"applsci-15-05785.pdf\"\n",
    "]\n",
    "\n",
    "print(\"📚 Building Scientific Paper Corpus...\")\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = Path(pdf_file) # Define pdf_path here\n",
    "\n",
    "    if not pdf_path.exists():\n",
    "        # Adjusted the default path...\n",
    "        potential_path = Path(\"/content\") / pdf_path.name\n",
    "        if potential_path.exists():\n",
    "             pdf_path = potential_path\n",
    "             print(f\"File not found at {pdf_file}, found at {potential_path}\") # Changed f-string to use pdf_file\n",
    "        else:\n",
    "             raise FileNotFoundError(f\"PDF file not found at {pdf_file} or {potential_path}\") # Changed f-string to use pdf_file\n",
    "\n",
    "    # Upload the PDF file\n",
    "    corpus_manager.upload_pdf(str(pdf_path)) # Call upload_pdf with the path\n",
    "\n",
    "# Build the corpus index\n",
    "corpus_manager.build_corpus_index()\n",
    "\n",
    "# Display corpus statistics\n",
    "stats = corpus_manager.get_corpus_stats()\n",
    "print(f\"\\n📊 Corpus Statistics:\")\n",
    "print(f\"   - Total Documents: {stats['total_documents']}\")\n",
    "print(f\"   - Total Chunks: {stats['total_chunks']}\")\n",
    "\n",
    "# Example scientist queries\n",
    "questions = [\n",
    "    \"What challenges did students face when using AI tools in software development?\",\n",
    "    \"How effective are AI tools in improving coding productivity?\",\n",
    "    \"What methodologies were used to evaluate student performance?\",\n",
    "    \"What are the main factors influencing AI tool adoption in education?\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔬 Answering Scientific Questions...\")\n",
    "for question in questions:\n",
    "    result = qa_system.answer_question(question)\n",
    "\n",
    "    print(f\"\\n❓ Question: {result['question']}\")\n",
    "    print(f\"✅ Answer: {result['answer']}\")\n",
    "    print(f\"📄 Sources: {result['num_sources']} documents\")\n",
    "    for source in result['sources'][:2]:  # Show top 2 sources\n",
    "        print(f\"   - {source['document']} (similarity: {source['similarity_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCCgZfddxjFp",
    "outputId": "b014e3e8-e2b9-4e7b-b3da-3b2c00e6528a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting Comprehensive Evaluation...\n",
      "\n",
      "📝 Evaluating question 1/3\n",
      "🔍 Searching corpus for: What methodology was used to collect data in the AI tools study?\n",
      "🤖 Generating answer with Groq...\n",
      "   EM: 0.000 | F1: 0.103 | Semantic: 0.390\n",
      "\n",
      "📝 Evaluating question 2/3\n",
      "🔍 Searching corpus for: What was the adoption rate of AI tools among students?\n",
      "🤖 Generating answer with Groq...\n",
      "   EM: 0.000 | F1: 0.606 | Semantic: 0.806\n",
      "\n",
      "📝 Evaluating question 3/3\n",
      "🔍 Searching corpus for: Which development phase had the highest AI tool usage?\n",
      "🤖 Generating answer with Groq...\n",
      "   EM: 0.000 | F1: 0.556 | Semantic: 0.629\n",
      "\n",
      "============================================================\n",
      "📊 SCIENTIFIC QA SYSTEM EVALUATION REPORT\n",
      "============================================================\n",
      "\n",
      "🎯 Overall Performance Metrics:\n",
      "   • Average Exact Match:      0.000\n",
      "   • Average F1 Score:         0.422\n",
      "   • Average Semantic Sim:     0.608\n",
      "   • Average Answer Relevancy: 0.795\n",
      "\n",
      "📈 Performance by Question Category:\n",
      "   • Methodology: F1=0.103, Semantic=0.390\n",
      "   • Statistics: F1=0.606, Semantic=0.806\n",
      "   • Findings: F1=0.556, Semantic=0.629\n",
      "\n",
      "✅ Best Performing Question:\n",
      "   Q: What was the adoption rate of AI tools among students?...\n",
      "   F1: 0.606\n",
      "\n",
      "❌ Worst Performing Question:\n",
      "   Q: What methodology was used to collect data in the AI tools study?...\n",
      "   F1: 0.103\n",
      "\n",
      "✨ Relevance Accuracy (Average Answer Relevancy Score): 0.795\n",
      "✨ Semantic Accuracy (Average Semantic Similarity Score): 0.608\n"
     ]
    }
   ],
   "source": [
    "# prompt: tell the relevance accuracy of this model\n",
    "\n",
    "evaluator = QAEvaluator(qa_system)\n",
    "\n",
    "# Run the evaluation\n",
    "results, avg_results = evaluator.run_comprehensive_evaluation()\n",
    "\n",
    "# Generate and print the report\n",
    "evaluation_report = evaluator.generate_evaluation_report(results, avg_results)\n",
    "\n",
    "# The 'Answer Relevancy' metric from the report is a measure of how relevant the generated answer is to the question,\n",
    "# based on semantic similarity. While not a direct \"accuracy\" measure against a ground truth answer's content,\n",
    "# it indicates the model's ability to produce an answer that stays on topic.\n",
    "print(f\"\\n✨ Relevance Accuracy (Average Answer Relevancy Score): {avg_results['avg_answer_relevancy']:.3f}\")\n",
    "\n",
    "# You could also consider semantic similarity to ground truth as a form of accuracy in terms of meaning\n",
    "print(f\"✨ Semantic Accuracy (Average Semantic Similarity Score): {avg_results['avg_semantic_similarity']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
